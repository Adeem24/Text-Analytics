{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6360eeed-bacd-4028-b22f-06420758584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [wonder, anyon, could, enlighten, door, sport,...\n",
      "1        [recent, post, articl, ask, kind, rate, singl,...\n",
      "2        [depend, prioriti, peopl, higher, prioriti, mi...\n",
      "3        [excel, automat, found, subaru, legaci, switch...\n",
      "4        [ford, automobil, need, inform, whether, ford,...\n",
      "                               ...                        \n",
      "11309    [secreci, clipper, chip, serial, number, clipp...\n",
      "11310    [interest, sourc, feal, encrypt, algorithm, so...\n",
      "11311    [actual, algorithm, classifi, howev, main, thr...\n",
      "11312    [appear, gener, call, upon, name, anti, christ...\n",
      "11313    [probabl, keep, quiet, take, lest, kneecap, bust]\n",
      "Name: tokens, Length: 11096, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Kamal Adeem Bin Kamaruddin IS01081937\n",
    "#Muhammad Hafiz SW01081229\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('news_dataset.csv')\n",
    "\n",
    "# Drop nulls and keep only 'text' column\n",
    "df = df[['text']].dropna()\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(stemmer.stem(word))\n",
    "        for word in tokens if word not in stop_words and len(word) > 3\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['tokens'] = df['text'].apply(preprocess)\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8653c830-720d-40bb-b498-b3c77bbe6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Coherence Score: 0.4944\n",
      "\n",
      "ðŸ“„ First few labeled articles:\n",
      "                                                text  assigned_topic\n",
      "0  I was wondering if anyone out there could enli...               1\n",
      "1  I recently posted an article asking what kind ...               0\n",
      "2  \\nIt depends on your priorities.  A lot of peo...               0\n",
      "3  an excellent automatic can be found in the sub...               1\n",
      "4  : Ford and his automobile.  I need information...               1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "\n",
    "# Train LDA Model with 4 topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=15, random_state=42)\n",
    "\n",
    "# Evaluate with Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=df['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"\\nðŸ§ª Coherence Score: {coherence_score:.4f}\\n\")\n",
    "\n",
    "# Assign dominant topic to each document\n",
    "df['assigned_topic'] = [\n",
    "    max(lda_model.get_document_topics(dictionary.doc2bow(tokens)), key=lambda x: x[1])[0]\n",
    "    for tokens in df['tokens']\n",
    "]\n",
    "\n",
    "# Show first few labeled articles\n",
    "print(\"ðŸ“„ First few labeled articles:\")\n",
    "print(df[['text', 'assigned_topic']].head(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9330056-11aa-4a22-929d-70612d700701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per topic:\n",
      "\n",
      "Topic 1:\n",
      "- would (weight: 0.008)\n",
      "- peopl (weight: 0.007)\n",
      "- think (weight: 0.005)\n",
      "- like (weight: 0.005)\n",
      "- know (weight: 0.005)\n",
      "- time (weight: 0.004)\n",
      "- year (weight: 0.004)\n",
      "- make (weight: 0.004)\n",
      "- right (weight: 0.004)\n",
      "- govern (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      "- use (weight: 0.008)\n",
      "- system (weight: 0.006)\n",
      "- file (weight: 0.006)\n",
      "- encrypt (weight: 0.006)\n",
      "- program (weight: 0.005)\n",
      "- chip (weight: 0.005)\n",
      "- window (weight: 0.004)\n",
      "- would (weight: 0.004)\n",
      "- like (weight: 0.004)\n",
      "- inform (weight: 0.004)\n"
     ]
    }
   ],
   "source": [
    "# Show top terms per topic\n",
    "print(\"Top terms per topic:\")\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for idx, topic in topics:\n",
    "    print(f\"\\nTopic {idx + 1}:\")\n",
    "    for term in topic.split('+'):\n",
    "        weight, word = term.split('*')\n",
    "        print(f\"- {word.strip().strip('\\\"')} (weight: {weight.strip()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a34617-a26e-4b59-9e35-9a727d713c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in Topic 2 above, words like use, system, file,\\nand encrypt appear together and suggest a theme \\naround technology or cybersecurity. The \\ntop words in topic 1 are unrelated or too generic, the coherence score may be lower, \\nsignaling that the topic might not represent a distinct, meaningful theme.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"in Topic 2 above, words like use, system, file,\n",
    "and encrypt appear together and suggest a theme \n",
    "around technology or cybersecurity. The \n",
    "top words in topic 1 are unrelated or too generic, the coherence score may be lower, \n",
    "signaling that the topic might not represent a distinct, meaningful theme.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23753e31-21c7-45d8-be65-d69bf2b3441d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
